{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pasamos Los Features del otro notebook\n",
    "\n",
    "oportunidades = pd.read_csv(\"Train_TP2_Datos_2020-2C.csv\")\n",
    "oportunidades['Account_Created_Date'] = pd.to_datetime(oportunidades['Account_Created_Date'])\n",
    "oportunidades['Opportunity_Created_Date'] = pd.to_datetime(oportunidades['Opportunity_Created_Date'])\n",
    "oportunidades['Quote_Expiry_Date'] = pd.to_datetime(oportunidades['Quote_Expiry_Date'])\n",
    "oportunidades['Last_Modified_Date'] = pd.to_datetime(oportunidades['Last_Modified_Date'])\n",
    "oportunidades['Planned_Delivery_Start_Date'] = pd.to_datetime(oportunidades['Planned_Delivery_Start_Date'])\n",
    "oportunidades['Planned_Delivery_End_Date'] = pd.to_datetime(oportunidades['Planned_Delivery_End_Date'])\n",
    "oportunidades_japon = (oportunidades.loc[oportunidades['Region'] == 'Japan'])\n",
    "oportunidades = (oportunidades.loc[oportunidades['Region'] != 'Japan'])\n",
    "#oportunidades_japon = oportunidades_japon.iloc[:,:3]\n",
    "oportunidades_japon['Territory'] = oportunidades_japon['Territory'].replace({'None':'Japan'})\n",
    "oportunidades = pd.concat([oportunidades, oportunidades_japon], axis=0)\n",
    "oportunidades['Region'] = oportunidades['Region'].replace({'Japan':'APAC', 'Middle East':'EMEA'})\n",
    "oportunidades[oportunidades.select_dtypes(['object']).columns] = oportunidades.select_dtypes(['object']).apply(lambda x: x.astype('category'))\n",
    "oportunidades = oportunidades.loc[oportunidades['Territory'] != 'None']\n",
    "oportunidades = oportunidades.dropna()\n",
    "\n",
    "filtro_terminos_entrega = oportunidades['Pricing, Delivery_Terms_Quote_Appr']\\\n",
    "     == oportunidades['Pricing, Delivery_Terms_Approved']\n",
    "\n",
    "filtro_codigo_burocratico = oportunidades['Bureaucratic_Code_0_Approval']\\\n",
    "     == oportunidades['Bureaucratic_Code_0_Approved']\n",
    "\n",
    "oportunidades = oportunidades.drop(['Pricing, Delivery_Terms_Quote_Appr'\\\n",
    "    ,'Pricing, Delivery_Terms_Approved', 'Bureaucratic_Code_0_Approval'\\\n",
    "        , 'Bureaucratic_Code_0_Approved', 'Submitted_for_Approval'], axis = 'columns')\n",
    "\n",
    "oportunidades_posibles = (filtro_terminos_entrega & filtro_codigo_burocratico)\n",
    "\n",
    "oportunidades.insert(3,'Es_Oportunidad_Posible', oportunidades_posibles)\n",
    "\n",
    "oportunidades['Es_Oportunidad_Posible'] = oportunidades['Es_Oportunidad_Posible'].replace(\n",
    "    {True:1, False:0})\n",
    "oportunidades.head()\n",
    "\n",
    "encoding_owners = oportunidades.groupby('Opportunity_Owner').agg({'Total_Amount':'mean'})\n",
    "encoding_owners.columns = ['Encoding_Vendedor']\n",
    "encoding_owners = encoding_owners.reset_index()\n",
    "oportunidades = oportunidades.merge(encoding_owners, how='inner', on='Opportunity_Owner')\n",
    "\n",
    "encoding_territory = oportunidades.groupby('Territory').agg({'Total_Amount' : 'mean'})\n",
    "encoding_territory.columns = ['Encoding_Territorio']\n",
    "encoding_territory = encoding_territory.reset_index()\n",
    "oportunidades = oportunidades.merge(encoding_territory, how='inner', on='Territory')\n",
    "\n",
    "oportunidades = pd.concat([oportunidades, pd.get_dummies(oportunidades['Region'])],axis='columns')\n",
    "\n",
    "encoding_moneda = oportunidades.groupby('Total_Amount_Currency').agg({'ASP' : 'mean'})\n",
    "encoding_moneda.columns = ['Encoding_Moneda']\n",
    "encoding_moneda = encoding_moneda.reset_index()\n",
    "oportunidades = oportunidades.merge(encoding_moneda, how='inner', on='Total_Amount_Currency')\n",
    "\n",
    "oportunidades = pd.concat([oportunidades, pd.get_dummies(oportunidades['Bureaucratic_Code'])], axis='columns')\n",
    "\n",
    "encoding_bill_c = oportunidades.groupby('Billing_Country').agg({'Total_Amount' : 'mean'})\n",
    "encoding_bill_c.columns = ['Encoding_Billing_Country']\n",
    "encoding_bill_c = encoding_bill_c.reset_index()\n",
    "oportunidades = oportunidades.merge(encoding_bill_c, how='inner', on='Billing_Country')\n",
    "\n",
    "encoding_product_family = oportunidades.groupby('Product_Family').agg({'Total_Amount' : 'mean'})\n",
    "encoding_product_family.columns = ['Encoding_Prod_Family']\n",
    "encoding_product_family = encoding_product_family.reset_index()\n",
    "oportunidades = oportunidades.merge(encoding_product_family, how='inner', on='Product_Family')\n",
    "\n",
    "oportunidades = pd.concat([oportunidades, pd.get_dummies(oportunidades['Account_Type'])],axis='columns')\n",
    "\n",
    "encoding_account_name = oportunidades.groupby('Account_Name').agg({'Total_Amount' : 'mean'})\n",
    "encoding_account_name.columns = ['Encoding_Acc_Name']\n",
    "encoding_account_name = encoding_account_name.reset_index()\n",
    "oportunidades = oportunidades.merge(encoding_account_name, how='inner', on='Account_Name')\n",
    "\n",
    "oportunidades = pd.concat([oportunidades,pd.get_dummies(oportunidades['Delivery_Terms'])],axis='columns')\n",
    "\n",
    "oportunidades['Opportunity_Created_Year'] = oportunidades['Opportunity_Created_Date'].dt.year\n",
    "oportunidades['Opportunity_Created_Month'] = oportunidades['Opportunity_Created_Date'].dt.month\n",
    "\n",
    "encoding_op_type = oportunidades.groupby('Opportunity_Type').agg({'Total_Taxable_Amount' : 'mean'})\n",
    "encoding_op_type.columns = ['Encoding_Opportunity_Type']\n",
    "encoding_op_type = encoding_op_type.reset_index()\n",
    "oportunidades = oportunidades.merge(encoding_op_type, how='inner', on='Opportunity_Type')\n",
    "\n",
    "oportunidades = pd.concat([oportunidades, pd.get_dummies(oportunidades['Source '])],axis = 'columns')\n",
    "\n",
    "y_train = oportunidades['Stage']\n",
    "oportunidades = oportunidades.loc[(oportunidades['Stage'] == 'Closed Won') | (oportunidades['Stage'] == 'Closed Lost')]\n",
    "oportunidades['Stage'] = oportunidades['Stage'].replace({'Closed Won': 1, 'Closed Lost': 0})\n",
    "std_deviation_amounts = oportunidades.groupby('Opportunity_ID').agg({'TRF':'std', 'Opportunity_Created_Month':'std'})\n",
    "std_deviation_amounts.columns = ['TRF_Std', 'Opp_Created_Month_Std']\n",
    "std_deviation_amounts = std_deviation_amounts.reset_index()\n",
    "\n",
    "#Solo usamos Bureau_Code_ 0,1,2,4,5 porque son los que tiene el set de test.\n",
    "x_train = oportunidades.groupby('Opportunity_ID').agg({'Opportunity_Created_Year' : 'mean',\\\n",
    "                                                       'Opportunity_Created_Month' : 'mean',\\\n",
    "                                                      'Encoding_Moneda':'mean', 'Total_Amount': 'sum',\\\n",
    "                                                      'Total_Taxable_Amount':'sum',\\\n",
    "                                                      'Encoding_Territorio':'mean','Encoding_Vendedor':'mean',\\\n",
    "                                                      'Es_Oportunidad_Posible':'mean','Stage':'max', 'TRF':'mean',\n",
    "                                                      'Bureaucratic_Code_0':'mean','Bureaucratic_Code_1':'mean',\\\n",
    "                                                      'Bureaucratic_Code_2':'mean','Bureaucratic_Code_4':'mean',\n",
    "                                                      'Bureaucratic_Code_5':'mean', 'Encoding_Billing_Country':'mean',\\\n",
    "                                                      'Delivery_Terms_0':'mean','Delivery_Terms_1':'mean','Delivery_Terms_2':'mean',\\\n",
    "                                                      'Delivery_Terms_3':'mean','Delivery_Terms_4':'mean','Delivery_Terms_5':'mean',\\\n",
    "                                                      'Delivery_Terms_6':'mean','Delivery_Terms_7':'mean','Delivery_Terms_8':'mean',\n",
    "                                                      'Encoding_Prod_Family':'mean','Account_Type_0':'mean','Account_Type_1':'mean',\\\n",
    "                                                      'Account_Type_2':'mean','Account_Type_4':'mean', 'Account_Type_5':'mean',\\\n",
    "                                                      'APAC' : 'mean','Americas':'mean','EMEA':'mean', 'Encoding_Acc_Name':'mean',\\\n",
    "                                                      'Encoding_Opportunity_Type':'mean', 'Source_3':'mean', 'Source_7':'mean',\\\n",
    "                                                      'Source_9':'mean','Source_11':'mean','Source_13':'mean'})\n",
    "x_train = x_train.reset_index()\n",
    "x_train = x_train.merge(std_deviation_amounts, on='Opportunity_ID', how='inner')\n",
    "x_train = x_train.set_index('Opportunity_ID')\n",
    "y_train = x_train['Stage']\n",
    "x_train = x_train.drop('Stage', axis='columns')\n",
    "x_train['TRF_Std'] = x_train['TRF_Std'].replace({np.nan:0})\n",
    "x_train['Opp_Created_Month_Std'] = x_train['Opp_Created_Month_Std'].replace({np.nan:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Division del set de test y train:\n",
    "\n",
    "division_x_train, division_x_test, division_y_train, division_y_test = train_test_split(x_train, y_train, test_size = 0.3,\\\n",
    "                                                                                       random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10500 candidates, totalling 21000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    8.3s\n",
      "exception calling callback for <Future at 0x210bbb36fd0 state=finished raised TerminatedWorkerError>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\_base.py\", line 625, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 347, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 780, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 531, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\reusable_executor.py\", line 177, in submit\n",
      "    return super(_ReusablePoolExecutor, self).submit(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 1102, in submit\n",
      "    raise self._flags.broken\n",
      "joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Tuning de Hiper parametros con GridSearchCV\n",
    "\n",
    "model = GridSearchCV(CatBoostRegressor(boosting_type='Plain',task_type=\"GPU\",gpu_ram_part=0.8, gpu_cat_features_storage='CpuPinnedMemory') ,{\n",
    "    'iterations' : [100,200,300,400,500],\n",
    "    'learning_rate' : np.arange(0.008, 0.022, 0.002),\n",
    "    #'border_count' : [2,3,4,5,6,7,8,9,10],\n",
    "    #'colsample_bytree' : np.arange(0.3,0.8,0.1),\n",
    "    #'alpha' : [0.0001,0.001,0.01],\n",
    "    'depth' : np.arange(5,11),\n",
    "    'l2_leaf_reg' : np.logspace(-20, 0),\n",
    "    #'gamma' : np.arange(0,0.3,0.05),\n",
    "    #'random_strength' : np.arange(0.5,1,0.1),\n",
    "}, cv = 2, return_train_score = False, n_jobs = -1, verbose=10)\n",
    "\n",
    "model.fit(division_x_train, division_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(division_x_test, division_y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
