{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Conv1D, Flatten, GlobalMaxPool1D, Reshape, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from kerastuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pasamos Los Features del otro notebook\n",
    "\n",
    "oportunidades = pd.read_csv(\"Train_TP2_Datos_2020-2C.csv\")\n",
    "oportunidades['Account_Created_Date'] = pd.to_datetime(oportunidades['Account_Created_Date'])\n",
    "oportunidades['Opportunity_Created_Date'] = pd.to_datetime(oportunidades['Opportunity_Created_Date'])\n",
    "oportunidades['Quote_Expiry_Date'] = pd.to_datetime(oportunidades['Quote_Expiry_Date'])\n",
    "oportunidades['Last_Modified_Date'] = pd.to_datetime(oportunidades['Last_Modified_Date'])\n",
    "oportunidades['Planned_Delivery_Start_Date'] = pd.to_datetime(oportunidades['Planned_Delivery_Start_Date'])\n",
    "oportunidades['Planned_Delivery_End_Date'] = pd.to_datetime(oportunidades['Planned_Delivery_End_Date'])\n",
    "oportunidades_japon = (oportunidades.loc[oportunidades['Region'] == 'Japan'])\n",
    "oportunidades = (oportunidades.loc[oportunidades['Region'] != 'Japan'])\n",
    "#oportunidades_japon = oportunidades_japon.iloc[:,:3]\n",
    "oportunidades_japon['Territory'] = oportunidades_japon['Territory'].replace({'None':'Japan'})\n",
    "oportunidades = pd.concat([oportunidades, oportunidades_japon], axis=0)\n",
    "oportunidades['Region'] = oportunidades['Region'].replace({'Japan':'APAC', 'Middle East':'EMEA'})\n",
    "oportunidades[oportunidades.select_dtypes(['object']).columns] = oportunidades.select_dtypes(['object']).apply(lambda x: x.astype('category'))\n",
    "oportunidades = oportunidades.loc[oportunidades['Territory'] != 'None']\n",
    "oportunidades = oportunidades.dropna()\n",
    "\n",
    "filtro_terminos_entrega = oportunidades['Pricing, Delivery_Terms_Quote_Appr']\\\n",
    "     == oportunidades['Pricing, Delivery_Terms_Approved']\n",
    "\n",
    "filtro_codigo_burocratico = oportunidades['Bureaucratic_Code_0_Approval']\\\n",
    "     == oportunidades['Bureaucratic_Code_0_Approved']\n",
    "\n",
    "oportunidades = oportunidades.drop(['Pricing, Delivery_Terms_Quote_Appr'\\\n",
    "    ,'Pricing, Delivery_Terms_Approved', 'Bureaucratic_Code_0_Approval'\\\n",
    "        , 'Bureaucratic_Code_0_Approved', 'Submitted_for_Approval'], axis = 'columns')\n",
    "\n",
    "oportunidades_posibles = (filtro_terminos_entrega & filtro_codigo_burocratico)\n",
    "\n",
    "oportunidades.insert(3,'Es_Oportunidad_Posible', oportunidades_posibles)\n",
    "\n",
    "oportunidades['Es_Oportunidad_Posible'] = oportunidades['Es_Oportunidad_Posible'].replace(\n",
    "    {True:1, False:0})\n",
    "oportunidades.head()\n",
    "\n",
    "encoding_owners = oportunidades.groupby('Opportunity_Owner').agg({'Total_Amount':'mean'})\n",
    "encoding_owners.columns = ['Encoding_Vendedor']\n",
    "encoding_owners = encoding_owners.reset_index()\n",
    "oportunidades = oportunidades.merge(encoding_owners, how='inner', on='Opportunity_Owner')\n",
    "\n",
    "encoding_territory = oportunidades.groupby('Territory').agg({'Total_Amount' : 'mean'})\n",
    "encoding_territory.columns = ['Encoding_Territorio']\n",
    "encoding_territory = encoding_territory.reset_index()\n",
    "oportunidades = oportunidades.merge(encoding_territory, how='inner', on='Territory')\n",
    "\n",
    "oportunidades = pd.concat([oportunidades, pd.get_dummies(oportunidades['Region'])],axis='columns')\n",
    "\n",
    "encoding_moneda = oportunidades.groupby('Total_Amount_Currency').agg({'ASP' : 'mean'})\n",
    "encoding_moneda.columns = ['Encoding_Moneda']\n",
    "encoding_moneda = encoding_moneda.reset_index()\n",
    "oportunidades = oportunidades.merge(encoding_moneda, how='inner', on='Total_Amount_Currency')\n",
    "\n",
    "oportunidades = pd.concat([oportunidades, pd.get_dummies(oportunidades['Bureaucratic_Code'])], axis='columns')\n",
    "\n",
    "encoding_bill_c = oportunidades.groupby('Billing_Country').agg({'Total_Amount' : 'mean'})\n",
    "encoding_bill_c.columns = ['Encoding_Billing_Country']\n",
    "encoding_bill_c = encoding_bill_c.reset_index()\n",
    "oportunidades = oportunidades.merge(encoding_bill_c, how='inner', on='Billing_Country')\n",
    "\n",
    "encoding_product_family = oportunidades.groupby('Product_Family').agg({'Total_Amount' : 'mean'})\n",
    "encoding_product_family.columns = ['Encoding_Prod_Family']\n",
    "encoding_product_family = encoding_product_family.reset_index()\n",
    "oportunidades = oportunidades.merge(encoding_product_family, how='inner', on='Product_Family')\n",
    "\n",
    "oportunidades = pd.concat([oportunidades, pd.get_dummies(oportunidades['Account_Type'])],axis='columns')\n",
    "\n",
    "encoding_account_name = oportunidades.groupby('Account_Name').agg({'Total_Amount' : 'mean'})\n",
    "encoding_account_name.columns = ['Encoding_Acc_Name']\n",
    "encoding_account_name = encoding_account_name.reset_index()\n",
    "oportunidades = oportunidades.merge(encoding_account_name, how='inner', on='Account_Name')\n",
    "\n",
    "oportunidades = pd.concat([oportunidades,pd.get_dummies(oportunidades['Delivery_Terms'])],axis='columns')\n",
    "\n",
    "oportunidades['Opportunity_Created_Year'] = oportunidades['Opportunity_Created_Date'].dt.year\n",
    "oportunidades['Opportunity_Created_Month'] = oportunidades['Opportunity_Created_Date'].dt.month\n",
    "\n",
    "encoding_op_type = oportunidades.groupby('Opportunity_Type').agg({'Total_Taxable_Amount' : 'mean'})\n",
    "encoding_op_type.columns = ['Encoding_Opportunity_Type']\n",
    "encoding_op_type = encoding_op_type.reset_index()\n",
    "oportunidades = oportunidades.merge(encoding_op_type, how='inner', on='Opportunity_Type')\n",
    "\n",
    "oportunidades = pd.concat([oportunidades, pd.get_dummies(oportunidades['Source '])],axis = 'columns')\n",
    "\n",
    "y_train = oportunidades['Stage']\n",
    "oportunidades = oportunidades.loc[(oportunidades['Stage'] == 'Closed Won') | (oportunidades['Stage'] == 'Closed Lost')]\n",
    "oportunidades['Stage'] = oportunidades['Stage'].replace({'Closed Won': 1, 'Closed Lost': 0})\n",
    "std_deviation_amounts = oportunidades.groupby('Opportunity_ID').agg({'TRF':'std', 'Opportunity_Created_Month':'std'})\n",
    "std_deviation_amounts.columns = ['TRF_Std', 'Opp_Created_Month_Std']\n",
    "std_deviation_amounts = std_deviation_amounts.reset_index()\n",
    "\n",
    "#Solo usamos Bureau_Code_ 0,1,2,4,5 porque son los que tiene el set de test.\n",
    "x_train = oportunidades.groupby('Opportunity_ID').agg({'Opportunity_Created_Year' : 'mean',\\\n",
    "                                                       'Opportunity_Created_Month' : 'mean',\\\n",
    "                                                      'Encoding_Moneda':'mean', 'Total_Amount': 'sum',\\\n",
    "                                                      'Total_Taxable_Amount':'sum',\\\n",
    "                                                      'Encoding_Territorio':'mean','Encoding_Vendedor':'mean',\\\n",
    "                                                      'Es_Oportunidad_Posible':'mean','Stage':'max', 'TRF':'mean',\n",
    "                                                      'Bureaucratic_Code_0':'mean','Bureaucratic_Code_1':'mean',\\\n",
    "                                                      'Bureaucratic_Code_2':'mean','Bureaucratic_Code_4':'mean',\n",
    "                                                      'Bureaucratic_Code_5':'mean', 'Encoding_Billing_Country':'mean',\\\n",
    "                                                      'Delivery_Terms_0':'mean','Delivery_Terms_1':'mean','Delivery_Terms_2':'mean',\\\n",
    "                                                      'Delivery_Terms_3':'mean','Delivery_Terms_4':'mean','Delivery_Terms_5':'mean',\\\n",
    "                                                      'Delivery_Terms_6':'mean','Delivery_Terms_7':'mean','Delivery_Terms_8':'mean',\n",
    "                                                      'Encoding_Prod_Family':'mean','Account_Type_0':'mean','Account_Type_1':'mean',\\\n",
    "                                                      'Account_Type_2':'mean','Account_Type_4':'mean', 'Account_Type_5':'mean',\\\n",
    "                                                      'APAC' : 'mean','Americas':'mean','EMEA':'mean', 'Encoding_Acc_Name':'mean',\\\n",
    "                                                      'Encoding_Opportunity_Type':'mean', 'Source_3':'mean', 'Source_7':'mean',\\\n",
    "                                                      'Source_9':'mean','Source_11':'mean','Source_13':'mean'})\n",
    "x_train = x_train.reset_index()\n",
    "x_train = x_train.merge(std_deviation_amounts, on='Opportunity_ID', how='inner')\n",
    "x_train = x_train.set_index('Opportunity_ID')\n",
    "y_train = x_train['Stage']\n",
    "x_train = x_train.drop('Stage', axis='columns')\n",
    "x_train['TRF_Std'] = x_train['TRF_Std'].replace({np.nan:0})\n",
    "x_train['Opp_Created_Month_Std'] = x_train['Opp_Created_Month_Std'].replace({np.nan:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos los modelos de XGBoost y CatBoost\n",
    "\n",
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':12,\n",
    "    'n_estimators':450,\n",
    "    'colsample_bytree':0.3,\n",
    "    'subsample':0.7,\n",
    "    'learning_rate':0.014,\n",
    "    'alpha':0.01,\n",
    "    'gamma':0.0\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':9,\n",
    "    'n_estimators':350,\n",
    "    'colsample_bytree':0.5,\n",
    "    'subsample':0.5,\n",
    "    'learning_rate':0.02,\n",
    "    'alpha':0.0001,\n",
    "    'gamma':0.1\n",
    "}\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective =params['objective'], \n",
    "                colsample_bytree = params['colsample_bytree'], learning_rate = params['learning_rate'],\n",
    "                max_depth = params['max_depth'], alpha = params['alpha'], n_estimators = params['n_estimators'], \n",
    "                subsample = params['subsample'], early_stopping_rounds=5)\n",
    "\n",
    "xg_reg2 = xgb.XGBRegressor(objective =params2['objective'], \n",
    "                colsample_bytree = params2['colsample_bytree'], learning_rate = params2['learning_rate'],\n",
    "                max_depth = params2['max_depth'], alpha = params2['alpha'], n_estimators = params2['n_estimators'], \n",
    "                subsample = params2['subsample'], early_stopping_rounds=5)\n",
    "\n",
    "model_xgb = xgb.XGBRegressor()\n",
    "model_catb = CatBoostRegressor(verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:49:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:49:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:49:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:49:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(alpha=0.0001, base_score=0.5, booster='gbtree',\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.5,\n",
       "             early_stopping_rounds=5, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.02, max_delta_step=0, max_depth=9,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=350, n_jobs=12, num_parallel_tree=1,\n",
       "             objective='binary:logistic', random_state=0,\n",
       "             reg_alpha=9.99999975e-05, reg_lambda=1, scale_pos_weight=1,\n",
       "             subsample=0.5, tree_method='exact', validate_parameters=1,\n",
       "             verbosity=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dividimos el set en train y test, y entrenamos ambos modelos\n",
    "\n",
    "division_x_train, division_x_test, division_y_train, division_y_test = train_test_split(x_train, y_train, test_size = 0.3,\\\n",
    "                                                                                       random_state = 123)\n",
    "model_xgb.fit(division_x_train, division_y_train)\n",
    "model_catb.fit(division_x_train, division_y_train)\n",
    "xg_reg.fit(division_x_train, division_y_train)\n",
    "xg_reg2.fit(division_x_train, division_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b0511fff7d01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Scoreamos con XGBoost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdivision_x_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_sets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdivision_y_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_xgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdivision_x_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscore_xgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_xgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdivision_x_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdivision_y_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlog_loss_xgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdivision_y_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'objective'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'none'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0malias\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_ConfigAliases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"num_iterations\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0malias\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m             \u001b[0mnum_boost_round\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Found `{}` in params. Will use it instead of argument\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "#Scoreamos con XGBoost\n",
    "predictions = model_xgb.predict(division_x_test)\n",
    "score_xgb = model_xgb.score(division_x_test, division_y_test)\n",
    "log_loss_xgb = log_loss(division_y_test, predictions)\n",
    "acc = accuracy_score(division_y_test, predictions.round())\n",
    "\n",
    "print(\"Score = \"+str(score_xgb)+\" ; Loss Original = \" + str(log_loss_xgb)+\" ; Accuracy = \"+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scoreamos con CatBoost\n",
    "acc = accuracy_score(division_y_test, model_catb.predict(division_x_test).round())\n",
    "score_cb = model_catb.score(division_x_test, division_y_test)\n",
    "loss_cb = log_loss(division_y_test, model_catb.predict(division_x_test))\n",
    "print(\"Score = \"+str(score_cb)+\" ; Loss Original = \" + str(loss_cb)+\" ; Accuracy = \"+str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scoreamos con XGBoost tuneado\n",
    "score = xg_reg.score(division_x_test, division_y_test)\n",
    "log_loss_original = log_loss(division_y_test, xg_reg.predict(division_x_test))\n",
    "array = xg_reg.predict(division_x_test)\n",
    "#array = pd.DataFrame(array)\n",
    "for i in range(len(array)):\n",
    "    if array[i] <= 0.025:\n",
    "        array[i] = 0.025\n",
    "    elif array[i] >= 0.975:\n",
    "        array[i] = 0.975\n",
    "log_loss_mod = log_loss(division_y_test, array)\n",
    "print(\"Score = \"+str(score)+\" ; Loss Original = \" + str(log_loss_original) + \" ; Loss modificado = \" + str(log_loss_mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con segundo XGBoost tuneado\n",
    "score2 = xg_reg2.score(division_x_test, division_y_test)\n",
    "log_loss_original2 = log_loss(division_y_test, xg_reg2.predict(division_x_test))\n",
    "array2 = xg_reg2.predict(division_x_test)\n",
    "#array = pd.DataFrame(array)\n",
    "for i in range(len(array2)):\n",
    "    if array2[i] <= 0.025:\n",
    "        array2[i] = 0.025\n",
    "    elif array2[i] >= 0.975:\n",
    "        array2[i] = 0.975\n",
    "log_loss_mod2 = log_loss(division_y_test, array2)\n",
    "print(\"Score = \"+str(score2)+\" ; Loss Original = \" + str(log_loss_original2) + \" ; Loss modificado = \" + str(log_loss_mod2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
